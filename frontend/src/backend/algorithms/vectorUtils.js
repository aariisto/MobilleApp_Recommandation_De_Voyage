/**
 * Convertit un BLOB SQLite en array de nombres (vecteur)
 * @param {Uint8Array|ArrayBuffer} blob - Les donn√©es BLOB du vecteur
 * @returns {number[]} - Le vecteur sous forme d'array
 */
export function blobToVector(blob) {
  if (!blob) return [];

  // Convertir en Uint8Array si n√©cessaire
  const uint8Array = blob instanceof Uint8Array ? blob : new Uint8Array(blob);

  // Cr√©er un DataView pour lire les float64
  const dataView = new DataView(
    uint8Array.buffer,
    uint8Array.byteOffset,
    uint8Array.byteLength
  );

  // Lire les valeurs float64 (8 octets chacun)
  const vector = [];
  for (let i = 0; i < uint8Array.length; i += 8) {
    vector.push(dataView.getFloat64(i, true)); // true = little-endian
  }

  return vector;
}

/**
 * Convertit un array de nombres en BLOB SQLite
 * @param {number[]} vector - Le vecteur sous forme d'array
 * @returns {Uint8Array} - Les donn√©es BLOB
 */
export function vectorToBlob(vector) {
  if (!Array.isArray(vector) || vector.length === 0) {
    return new Uint8Array(0);
  }

  // Cr√©er un ArrayBuffer pour 8 octets par nombre (float64)
  const buffer = new ArrayBuffer(vector.length * 8);
  const dataView = new DataView(buffer);

  // √âcrire chaque nombre en float64 (little-endian)
  for (let i = 0; i < vector.length; i++) {
    dataView.setFloat64(i * 8, vector[i], true); // true = little-endian
  }

  return new Uint8Array(buffer);
}

/**
 * Calcule la similitude cosinus entre deux vecteurs
 * @param {number[]} vec1 - Premier vecteur
 * @param {number[]} vec2 - Deuxi√®me vecteur
 * @returns {number} - Similitude cosinus (0-1)
 */
export function cosineSimilarity(vec1, vec2) {
  if (!vec1 || !vec2 || vec1.length !== vec2.length || vec1.length === 0) {
    return 0;
  }

  let dotProduct = 0;
  let norm1 = 0;
  let norm2 = 0;

  for (let i = 0; i < vec1.length; i++) {
    dotProduct += vec1[i] * vec2[i];
    norm1 += vec1[i] * vec1[i];
    norm2 += vec2[i] * vec2[i];
  }

  norm1 = Math.sqrt(norm1);
  norm2 = Math.sqrt(norm2);

  if (norm1 === 0 || norm2 === 0) {
    return 0;
  }

  return dotProduct / (norm1 * norm2);
}

import { InferenceSession, Tensor } from "onnxruntime-react-native";
import { Asset } from "expo-asset";

// Import du vocabulaire JSON directement (g√©n√©r√© √† partir de vocab.txt)
const VOCAB_LIST = require("../../../assets/models/vocab.json");

// Cr√©ation de la map token -> ID au chargement
const vocabMap = new Map();
VOCAB_LIST.forEach((token, index) => {
  vocabMap.set(token, index);
});

console.log(`üìö Vocabulaire charg√©: ${vocabMap.size} tokens`);

// IDs des tokens sp√©ciaux
const CLS_TOKEN_ID = vocabMap.get("[CLS]") || 101;
const SEP_TOKEN_ID = vocabMap.get("[SEP]") || 102;
const PAD_TOKEN_ID = vocabMap.get("[PAD]") || 0;
const UNK_TOKEN_ID = vocabMap.get("[UNK]") || 100;

/**
 * Tokenize un texte en utilisant WordPiece
 * @param {string} text - Le texte √† tokenizer
 * @returns {number[]} - Liste des IDs de tokens
 */
function tokenizeText(text) {
  const tokens = [];

  // Ajouter [CLS] au d√©but
  tokens.push(CLS_TOKEN_ID);

  // Nettoyer et d√©couper le texte
  // On s√©pare la ponctuation pour matcher le comportement du tokenizer BERT (BasicTokenizer)
  // On remplace les points, underscores, tirets et autres signes par " . " pour qu'ils soient trait√©s comme des tokens s√©par√©s
  const normalizedText = text
    .toLowerCase()
    .replace(/([.,!?;:_\\-])/g, " $1 ") // Ajoute des espaces autour de la ponctuation (y compris _ et -)
    .trim();

  const words = normalizedText
    .split(/\s+/) // S√©pare par espaces
    .filter((w) => w.length > 0);

  for (const word of words) {
    // Essayer de trouver le mot complet d'abord
    if (vocabMap.has(word)) {
      tokens.push(vocabMap.get(word));
      continue;
    }

    // Sinon, faire du WordPiece (d√©couper en sous-parties)
    let start = 0;
    const wordTokens = [];
    let isBad = false;

    while (start < word.length) {
      let end = word.length;
      let foundToken = null;
      let foundEnd = start;

      // Chercher la plus longue sous-cha√Æne pr√©sente dans le vocab (greedy)
      while (end > start) {
        let substr = word.substring(start, end);

        // Ajouter "##" pour les sous-mots (sauf le premier)
        if (start > 0) {
          substr = "##" + substr;
        }

        if (vocabMap.has(substr)) {
          foundToken = substr;
          foundEnd = end;
          break;
        }

        end--;
      }

      if (foundToken !== null) {
        wordTokens.push(vocabMap.get(foundToken));
        start = foundEnd;
      } else {
        // Aucune correspondance trouv√©e, utiliser [UNK]
        isBad = true;
        break;
      }
    }

    if (isBad) {
      wordTokens.push(UNK_TOKEN_ID);
    } else {
      tokens.push(...wordTokens);
    }
  }

  // Ajouter [SEP] √† la fin
  tokens.push(SEP_TOKEN_ID);

  return tokens;
}

/**
 * G√©n√®re un embedding √† partir d'un texte en utilisant ONNX Runtime et le mod√®le MiniLM local
 * @param {string} text - Le texte √† encoder
 * @returns {Promise<number[]>} - L'embedding sous forme de tableau (384 dimensions)
 */
export async function generateEmbeddingLocal(text) {
  try {
    console.log("üìù Tokenization du texte:", text);

    // Tokenization avec notre impl√©mentation WordPiece
    const inputIds = tokenizeText(text);

    console.log(`üî¢ Tokens g√©n√©r√©s: ${inputIds.length}`);
    console.log(`üî¢ Input IDs complets: [${inputIds.slice(0, 20).join(", ")}]`);

    // D√©coder les tokens pour v√©rification
    const decodedTokens = inputIds.slice(0, 10).map((id) => {
      for (const [token, tokenId] of vocabMap.entries()) {
        if (tokenId === id) return token;
      }
      return `ID:${id}`;
    });
    console.log(`üî§ Tokens d√©cod√©s: [${decodedTokens.join(", ")}]`);

    // Padding √† une longueur fixe (128 tokens max pour MiniLM)
    const maxLength = 128;
    const paddedIds = [...inputIds];

    while (paddedIds.length < maxLength) {
      paddedIds.push(PAD_TOKEN_ID);
    }
    if (paddedIds.length > maxLength) {
      paddedIds.length = maxLength;
    }

    // Cr√©er le masque d'attention (1 pour tokens r√©els, 0 pour padding)
    const attentionMask = paddedIds.map((id) => (id !== PAD_TOKEN_ID ? 1 : 0));

    console.log("üîß Chargement du mod√®le ONNX...");
    const asset = Asset.fromModule(
      require("../../../assets/models/model_qint8_arm64.onnx")
    );
    await asset.downloadAsync();
    const session = await InferenceSession.create(asset.localUri);
    console.log("‚úÖ Mod√®le charg√©!");

    // Cr√©er les tensors ONNX avec BigInt64Array (requis pour int64)
    const inputIdsTensor = new Tensor(
      "int64",
      BigInt64Array.from(paddedIds.map(BigInt)),
      [1, maxLength]
    );

    const attentionMaskTensor = new Tensor(
      "int64",
      BigInt64Array.from(attentionMask.map(BigInt)),
      [1, maxLength]
    );

    // Cr√©er le tensor token_type_ids (tous √† 0 pour une seule phrase)
    const tokenTypeIdsTensor = new Tensor(
      "int64",
      BigInt64Array.from(new Array(maxLength).fill(0n)),
      [1, maxLength]
    );

    console.log("üöÄ Ex√©cution de l'inf√©rence ONNX...");

    // Ex√©cuter l'inf√©rence
    const output = await session.run({
      input_ids: inputIdsTensor,
      attention_mask: attentionMaskTensor,
      token_type_ids: tokenTypeIdsTensor,
    });

    console.log("‚úÖ Inf√©rence termin√©e!");

    // Le mod√®le peut retourner directement sentence_embedding ou last_hidden_state
    let embedding;

    if (output.sentence_embedding) {
      // Si le mod√®le retourne directement l'embedding pool√©
      embedding = Array.from(output.sentence_embedding.data);
      console.log(`üìä Embedding direct: ${embedding.length} dimensions`);
    } else if (output.last_hidden_state) {
      // Sinon, faire mean pooling sur last_hidden_state
      const outputData = output.last_hidden_state.data;
      const embeddingSize = 384;
      const seqLength = inputIds.length;

      embedding = new Array(embeddingSize).fill(0);
      let validTokens = 0;

      for (let i = 0; i < seqLength; i++) {
        if (attentionMask[i] === 1) {
          for (let j = 0; j < embeddingSize; j++) {
            embedding[j] += outputData[i * embeddingSize + j];
          }
          validTokens++;
        }
      }

      // Moyenne
      for (let i = 0; i < embeddingSize; i++) {
        embedding[i] /= validTokens;
      }

      console.log(`üìä Mean pooling: ${embedding.length} dimensions`);
    } else {
      throw new Error("Format de sortie ONNX non reconnu");
    }

    // Normalisation L2 (comme sentence-transformers en Python)
    let norm = 0;
    for (let i = 0; i < embedding.length; i++) {
      norm += embedding[i] * embedding[i];
    }
    norm = Math.sqrt(norm);

    // Diviser chaque valeur par la norme pour normaliser
    for (let i = 0; i < embedding.length; i++) {
      embedding[i] /= norm;
    }

    console.log(
      `‚úÖ Embedding g√©n√©r√©: ${embedding.length} dimensions (normalis√©)`
    );
    console.log(
      `üìä Premi√®res valeurs: ${embedding
        .slice(0, 5)
        .map((v) => v.toFixed(4))
        .join(", ")}`
    );

    return embedding;
  } catch (error) {
    console.error("‚ùå Erreur g√©n√©ration embedding:", error);
    throw error;
  }
}
